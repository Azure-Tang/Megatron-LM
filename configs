TransformerConfig(tensor_model_parallel_size=2,
pipeline_model_parallel_size=2,
virtual_pipeline_model_parallel_size=None,
sequence_parallel=True,
perform_initialization=True,
use_cpu_initialization=None,
fp16=True,
bf16=False,
params_dtype=torch.float16,
timers=None,
gradient_accumulation_fusion=True,
async_tensor_model_parallel_allreduce=False,
pipeline_dtype=torch.float16,
grad_scale_func=None,
enable_autocast=False,
autocast_dtype=torch.float16,
variable_seq_lengths=False,
num_microbatches_with_partial_activation_checkpoints=None,
overlap_p2p_comm=False,
batch_p2p_comm=True,
batch_p2p_sync=True,
use_ring_exchange_p2p=False,
deallocate_pipeline_outputs=True,
no_sync_func=None,
grad_sync_func=None,
param_sync_func=None,
num_layers=24,
hidden_size=1024,
num_attention_heads=16,
num_query_groups=16,
ffn_hidden_size=4096,
kv_channels=64,
hidden_dropout=0.1,
attention_dropout=0.1,
fp32_residual_connection=False,
apply_residual_connection_post_layernorm=False,
layernorm_epsilon=1e-05,
layernorm_zero_centered_gamma=False,
add_bias_linear=True,
gated_linear_unit=False,
activation_func=<built-in function gelu>,
init_method=<function init_method_normal.<locals>.init_ at 0x7f543e09e0d0>,
output_layer_init_method=<function scaled_init_method_normal.<locals>.init_ at 0x7f543e09e160>,
init_method_std=0.02,
apply_query_key_layer_scaling=True,
attention_softmax_in_fp32=True,
bias_gelu_fusion=True,
masked_softmax_fusion=True,
persist_layer_norm=True,
bias_dropout_fusion=True,
recompute_granularity=None,
recompute_method=None,
recompute_num_layers=None,
distribute_saved_activations=False,
fp8=None,
fp8_margin=0,
fp8_interval=1,
fp8_amax_history_len=1,
fp8_amax_compute_algo='most_recent',
fp8_wgrad=True,
normalization='LayerNorm')

tokens, position_ids, attention_mask, 
inputs shape: torch.Size([4, 1024])
inputs shape: torch.Size([4, 1024])
inputs shape: torch.Size([1, 1, 1024, 1024])